train.xgboost<-function(train.data){
param <- list("objective" = "binary:logistic",
"eval_metric" = "mlogloss",
'eta' = 0.3, 'max_depth' = 4)
nround<-500
trainnn<-as.matrix(train.data[,-1])
m <- mapply(trainnn, FUN=as.numeric)
m <- matrix(data=m, nrow=4000)
dtrain=xgb.DMatrix(data=m[,-1],label=m[,1])
bst<-xgb.train(data = dtrain,  param = param, nrounds = nround)
return(bst)
}
test.xgboost<-function(bst,test.data){
testtt<-as.matrix(test.data[,-1])
m.test <- mapply(testtt, FUN=as.numeric)
m.test <- matrix(data=m.test, nrow=1000)
xgboost.pred<-predict(bst, newdata = m.test[,-1])
return(xgboost.pred)
}
train.xgboost(train)
pred<-test.xgboost(train.xgboost,test)
pred<-test.xgboost(bst,test)
#load data
load("../data/new3_data_all.RData")
data.all = data.all[,-c(8:11)]
##############################################
####Tune model and choose parameters
library(xgboost)
#split 80% of the data to train set and 20% to test set
smp_size <- floor(0.8 * nrow(data.all))
set.seed(123)
train_ind <- sample(seq_len(nrow(data.all)), size = smp_size)
train <- data.all[train_ind, ]
test <- data.all[-train_ind, ]
labels <- train$is_churn
test_label <- test$is_churn
trainnn<-as.matrix(train[,-1])
testtt<-as.matrix(test[,-1])
m <- mapply(trainnn, FUN=as.numeric)
m <- matrix(data=m, nrow=4000)
m.test <- mapply(testtt, FUN=as.numeric)
m.test <- matrix(data=m.test, nrow=1000)
dtrain=xgb.DMatrix(data=m[,-1],label=m[,1])
NROUNDS = c(500,1000)
ETA = c(0.3)
MAX_DEPTH = c(3,4,5,6)
cv.xgb <- function(X.train, y.train, K, NROUNDS, ETA, MAX_DEPTH){
for (nround in NROUNDS){
for (eta in ETA){
for (max_depth in MAX_DEPTH){
n <- length(y.train)
n.fold <- floor(n/K)
s <- sample(rep(1:K, c(rep(n.fold, K-1), n-(K-1)*n.fold)))
cv.loss <- rep(NA, K)
for (i in 1:K){
train.data <- X.train[s != i,]
train.label <- y.train[s != i]
test.data <- X.train[s == i,]
test.label <- y.train[s == i]
param <- list("objective" = "binary:logistic",
"eval_metric" = "mlogloss",
'eta' = eta, 'max_depth' = max_depth)
dtrain=xgb.DMatrix(data=train.data,label=train.label)
bst <- xgb.train(data = dtrain,  param = param, nrounds = nround)
pred <- predict(bst, newdata = test.data)
#cv.acc[i] <- mean(pred == test.label)
cv.loss[i]<-LogLossBinary(test.label,pred)
}
print(paste("------Mean 5-fold cv loss for nround=",nround,",eta=",eta,",max_depth=",max_depth,
"------",mean(cv.loss)))
#key = c(nround,eta,max_depth)
#CV_ERRORS[key] = mean(cv.loss)
}
}
}
}
LogLossBinary <- function(actual, predicted, eps = 1e-15) {
predicted <- pmin(pmax(predicted, eps), 1-eps)
error<- - (sum(actual * log(predicted) + (1 - actual) * log(1 - predicted))) / length(actual)
return(error)
}
#CV_ERRORS = list()
cv.xgb(m[,-1],m[,1], 5, NROUNDS, ETA, MAX_DEPTH)
#####Tuned xgboost model
train.xgboost(train)
#####Tuned xgboost model
source("./xgboost_train&test.R")
train.xgboost(train)
train.xgboost(train)
#####Tuned xgboost model
source("./xgboost_train&test.R")
train.xgboost(train)
#####Tuned xgboost model
source("./xgboost_train&test.R")
train.xgboost(train)
xgmodel = train.xgboost(train)
pred<-test.xgboost(xgmodel, test)
test.label<-as.numeric(test[,2])-1
LogLossBinary(test.label,pred)
print(LogLossBinary(test.label,pred))
library(adabag)
load("./data/new3_data_all.Rdata")
load("../data/new3_data_all.Rdata")
str(data.all)
#split 80% of the data to train set and 20% to test set
smp_size <- floor(0.8 * nrow(data.all))
set.seed(123)
train_ind <- sample(seq_len(nrow(data.all)), size = smp_size)
train <- data.all[train_ind, ]
test <- data.all[-train_ind, ]
row.names(train)<-train[,1]
train<-train[,-1]
### Adaboost
# ntree = 20
adaboost20 <- boosting(is_churn~.,data = train, boos = T, mfinal = 20, coeflearn = "Breiman")
library(adabag)
install.packages("adabag")
### Adaboost
# ntree = 20
adaboost20 <- boosting(is_churn~.,data = train, boos = T, mfinal = 20, coeflearn = "Breiman")
load("../data/new3_data_all.Rdata")
str(data.all)
#split 80% of the data to train set and 20% to test set
smp_size <- floor(0.8 * nrow(data.all))
set.seed(123)
train_ind <- sample(seq_len(nrow(data.all)), size = smp_size)
train <- data.all[train_ind, ]
test <- data.all[-train_ind, ]
row.names(train)<-train[,1]
train<-train[,-1]
### Adaboost
# ntree = 20
adaboost20 <- boosting(is_churn~.,data = train, boos = T, mfinal = 20, coeflearn = "Breiman")
pred <- predict.boosting(adaboost20,newdata = test[,-1])
library(adabag)
load("../data/new3_data_all.Rdata")
str(data.all)
#split 80% of the data to train set and 20% to test set
smp_size <- floor(0.8 * nrow(data.all))
set.seed(123)
train_ind <- sample(seq_len(nrow(data.all)), size = smp_size)
train <- data.all[train_ind, ]
test <- data.all[-train_ind, ]
row.names(train)<-train[,1]
train<-train[,-1]
### Adaboost
# ntree = 20
adaboost20 <- boosting(is_churn~.,data = train, boos = T, mfinal = 20, coeflearn = "Breiman")
pred <- predict.boosting(adaboost20,newdata = test[,-1])
View(data.all)
#load data
load("../data/new3_data_all.RData")
data.all = data.all[,-c(7:11)]
##############################################
####Tune model and choose parameters
library(xgboost)
#split 80% of the data to train set and 20% to test set
smp_size <- floor(0.8 * nrow(data.all))
set.seed(123)
train_ind <- sample(seq_len(nrow(data.all)), size = smp_size)
train <- data.all[train_ind, ]
test <- data.all[-train_ind, ]
labels <- train$is_churn
test_label <- test$is_churn
trainnn<-as.matrix(train[,-1])
testtt<-as.matrix(test[,-1])
m <- mapply(trainnn, FUN=as.numeric)
m <- matrix(data=m, nrow=4000)
m.test <- mapply(testtt, FUN=as.numeric)
m.test <- matrix(data=m.test, nrow=1000)
dtrain=xgb.DMatrix(data=m[,-1],label=m[,1])
NROUNDS = c(500,1000)
ETA = c(0.3)
MAX_DEPTH = c(3,4,5,6)
cv.xgb <- function(X.train, y.train, K, NROUNDS, ETA, MAX_DEPTH){
for (nround in NROUNDS){
for (eta in ETA){
for (max_depth in MAX_DEPTH){
n <- length(y.train)
n.fold <- floor(n/K)
s <- sample(rep(1:K, c(rep(n.fold, K-1), n-(K-1)*n.fold)))
cv.loss <- rep(NA, K)
for (i in 1:K){
train.data <- X.train[s != i,]
train.label <- y.train[s != i]
test.data <- X.train[s == i,]
test.label <- y.train[s == i]
param <- list("objective" = "binary:logistic",
"eval_metric" = "mlogloss",
'eta' = eta, 'max_depth' = max_depth)
dtrain=xgb.DMatrix(data=train.data,label=train.label)
bst <- xgb.train(data = dtrain,  param = param, nrounds = nround)
pred <- predict(bst, newdata = test.data)
#cv.acc[i] <- mean(pred == test.label)
cv.loss[i]<-LogLossBinary(test.label,pred)
}
print(paste("------Mean 5-fold cv loss for nround=",nround,",eta=",eta,",max_depth=",max_depth,
"------",mean(cv.loss)))
#key = c(nround,eta,max_depth)
#CV_ERRORS[key] = mean(cv.loss)
}
}
}
}
LogLossBinary <- function(actual, predicted, eps = 1e-15) {
predicted <- pmin(pmax(predicted, eps), 1-eps)
error<- - (sum(actual * log(predicted) + (1 - actual) * log(1 - predicted))) / length(actual)
return(error)
}
#CV_ERRORS = list()
cv.xgb(m[,-1],m[,1], 5, NROUNDS, ETA, MAX_DEPTH)
#####Tuned xgboost model
source("./xgboost_train&test.R")
#####Tuned xgboost model
source("./xgboost_train&test.R")
#####Tuned xgboost model
source("../lib/xgboost_train&test.R")
#####Tuned xgboost model
source("../lib/no_member_data_version/xgboost_train&test.R")
#####Tuned xgboost model
source("../lib/no_member_data_version/xgboost/xgboost_train&test_nomember.R")
xgmodel = train.xgboost(train)
pred<-test.xgboost(xgmodel, test)
test.label<-as.numeric(test[,2])-1
print(LogLossBinary(test.label,pred))
setwd("~/Documents/GitHub/fall2017-project5-proj5-grp2/lib/no_member_data_version/xgboost")
#load data
# set wd at the file location
load("../../../data/new3_data_all.RData")
data.all = data.all[,-c(7:11)]
##############################################
####Tune model and choose parameters
library(xgboost)
#split 80% of the data to train set and 20% to test set
smp_size <- floor(0.8 * nrow(data.all))
set.seed(123)
train_ind <- sample(seq_len(nrow(data.all)), size = smp_size)
train <- data.all[train_ind, ]
test <- data.all[-train_ind, ]
labels <- train$is_churn
test_label <- test$is_churn
trainnn<-as.matrix(train[,-1])
testtt<-as.matrix(test[,-1])
m <- mapply(trainnn, FUN=as.numeric)
m <- matrix(data=m, nrow=4000)
m.test <- mapply(testtt, FUN=as.numeric)
m.test <- matrix(data=m.test, nrow=1000)
dtrain=xgb.DMatrix(data=m[,-1],label=m[,1])
NROUNDS = c(500,1000)
ETA = c(0.3)
MAX_DEPTH = c(3,4,5,6)
cv.xgb <- function(X.train, y.train, K, NROUNDS, ETA, MAX_DEPTH){
for (nround in NROUNDS){
for (eta in ETA){
for (max_depth in MAX_DEPTH){
n <- length(y.train)
n.fold <- floor(n/K)
s <- sample(rep(1:K, c(rep(n.fold, K-1), n-(K-1)*n.fold)))
cv.loss <- rep(NA, K)
for (i in 1:K){
train.data <- X.train[s != i,]
train.label <- y.train[s != i]
test.data <- X.train[s == i,]
test.label <- y.train[s == i]
param <- list("objective" = "binary:logistic",
"eval_metric" = "mlogloss",
'eta' = eta, 'max_depth' = max_depth)
dtrain=xgb.DMatrix(data=train.data,label=train.label)
bst <- xgb.train(data = dtrain,  param = param, nrounds = nround)
pred <- predict(bst, newdata = test.data)
#cv.acc[i] <- mean(pred == test.label)
cv.loss[i]<-LogLossBinary(test.label,pred)
}
print(paste("------Mean 5-fold cv loss for nround=",nround,",eta=",eta,",max_depth=",max_depth,
"------",mean(cv.loss)))
#key = c(nround,eta,max_depth)
#CV_ERRORS[key] = mean(cv.loss)
}
}
}
}
LogLossBinary <- function(actual, predicted, eps = 1e-15) {
predicted <- pmin(pmax(predicted, eps), 1-eps)
error<- - (sum(actual * log(predicted) + (1 - actual) * log(1 - predicted))) / length(actual)
return(error)
}
#CV_ERRORS = list()
cv.xgb(m[,-1],m[,1], 5, NROUNDS, ETA, MAX_DEPTH)
#####Tuned xgboost model
source("./xgboost_train&test_nomember.R")
xgmodel = train.xgboost(train)
pred<-test.xgboost(xgmodel, test)
test.label<-as.numeric(test[,2])-1
print(LogLossBinary(test.label,pred))
data.all = data.all[,-7:11]
library(adabag)
load("../data/new3_data_all.Rdata")
setwd("~/Documents/GitHub/fall2017-project5-proj5-grp2/lib/no_member_data_version/adaboost")
load("../../../data/new3_data_all.Rdata")
data.all = data.all[,-7:11]
data.all = data.all[,-(7:11)]
View(data.all)
str(data.all)
#split 80% of the data to train set and 20% to test set
smp_size <- floor(0.8 * nrow(data.all))
set.seed(123)
train_ind <- sample(seq_len(nrow(data.all)), size = smp_size)
train <- data.all[train_ind, ]
test <- data.all[-train_ind, ]
row.names(train)<-train[,1]
train<-train[,-1]
### Adaboost
# ntree = 20
adaboost20 <- boosting(is_churn~.,data = train, boos = T, mfinal = 20, coeflearn = "Breiman")
pred <- predict.boosting(adaboost20,newdata = test[,-1])
#ntree = 30
adaboost30 <- boosting(is_churn~.,data = train, boos = T, mfinal = 30, coeflearn = "Breiman")
pred2 <- predict.boosting(adaboost30, newdata = test[,-1])
#ntree = 50
adaboost50 <- boosting(is_churn~.,data = train, boos = T, mfinal = 50, coeflearn = "Breiman")
pred3 <- predict.boosting(adaboost50, newdata = test[,-1])
#ntree = 70
adaboost70 <- boosting(is_churn~., data = train, boos = T, mfinal = 70, coeflearn = "Breiman")
pred4 <- predict.boosting(adaboost70, newdata = test[,-1])
ada.cv20 <- boosting.cv(is_churn~.,data = train, v=5, mfinal = 20,
coeflearn = "Breiman",control = rpart.control(cp=0.01))
ada.cv30 <- boosting.cv(is_churn~.,data = train, v=5, mfinal = 30,
coeflearn = "Breiman",control = rpart.control(cp=0.01))
ada.cv50 <- boosting.cv(is_churn~.,data = train, v=5, mfinal = 50,
coeflearn = "Breiman",control = rpart.control(cp=0.01))
ada.cv70 <- boosting.cv(is_churn~., data = train, v=5, mfinal = 70,
coeflearn = "Breiman", control = rpart.control(cp=0.01))
ada.cv20$error #0.03125
ada.cv30$error #0.02825
ada.cv50$error #0.0285
ada.cv70$error #0.02925
pred$error #0.029
pred2$error #0.023
pred3$error #0.016
pred4$error #0.021
LogLossBinary <- function(actual, predicted, eps = 1e-15) {
predicted <- pmin(pmax(predicted, eps), 1-eps)
error<- - (sum(actual * log(predicted) + (1 - actual) * log(1 - predicted))) / length(actual)
return(error)
}
LogLossBinary(as.numeric(as.character(test[,2])),pred$prob[,2])
LogLossBinary(as.numeric(as.character(test[,2])),pred2$prob[,2])
LogLossBinary(as.numeric(as.character(test[,2])),pred3$prob[,2])
LogLossBinary(as.numeric(as.character(test[,2])),pred4$prob[,2])
#load data
# set wd at the file location
load("../../../data/new3_data_all.RData")
#load data
# set wd at the file location
load("../../../data/new3_data_all.RData")
View(data.all)
data.all = data.all[,-c(12:19)]
##############################################
####Tune model and choose parameters
library(xgboost)
#split 80% of the data to train set and 20% to test set
smp_size <- floor(0.8 * nrow(data.all))
set.seed(123)
train_ind <- sample(seq_len(nrow(data.all)), size = smp_size)
train <- data.all[train_ind, ]
test <- data.all[-train_ind, ]
labels <- train$is_churn
test_label <- test$is_churn
trainnn<-as.matrix(train[,-1])
testtt<-as.matrix(test[,-1])
m <- mapply(trainnn, FUN=as.numeric)
m <- matrix(data=m, nrow=4000)
m.test <- mapply(testtt, FUN=as.numeric)
m.test <- matrix(data=m.test, nrow=1000)
dtrain=xgb.DMatrix(data=m[,-1],label=m[,1])
NROUNDS = c(500,1000)
ETA = c(0.3)
MAX_DEPTH = c(3,4,5,6)
cv.xgb <- function(X.train, y.train, K, NROUNDS, ETA, MAX_DEPTH){
for (nround in NROUNDS){
for (eta in ETA){
for (max_depth in MAX_DEPTH){
n <- length(y.train)
n.fold <- floor(n/K)
s <- sample(rep(1:K, c(rep(n.fold, K-1), n-(K-1)*n.fold)))
cv.loss <- rep(NA, K)
for (i in 1:K){
train.data <- X.train[s != i,]
train.label <- y.train[s != i]
test.data <- X.train[s == i,]
test.label <- y.train[s == i]
param <- list("objective" = "binary:logistic",
"eval_metric" = "mlogloss",
'eta' = eta, 'max_depth' = max_depth)
dtrain=xgb.DMatrix(data=train.data,label=train.label)
bst <- xgb.train(data = dtrain,  param = param, nrounds = nround)
pred <- predict(bst, newdata = test.data)
#cv.acc[i] <- mean(pred == test.label)
cv.loss[i]<-LogLossBinary(test.label,pred)
}
print(paste("------Mean 5-fold cv loss for nround=",nround,",eta=",eta,",max_depth=",max_depth,
"------",mean(cv.loss)))
#key = c(nround,eta,max_depth)
#CV_ERRORS[key] = mean(cv.loss)
}
}
}
}
LogLossBinary <- function(actual, predicted, eps = 1e-15) {
predicted <- pmin(pmax(predicted, eps), 1-eps)
error<- - (sum(actual * log(predicted) + (1 - actual) * log(1 - predicted))) / length(actual)
return(error)
}
#CV_ERRORS = list()
cv.xgb(m[,-1],m[,1], 5, NROUNDS, ETA, MAX_DEPTH)
setwd("~/Documents/GitHub/fall2017-project5-proj5-grp2/lib/no_log_data_version/xgboost")
#####Tuned xgboost model
source("./xgboost_train&test_nolog.R")
xgmodel = train.xgboost(train)
pred<-test.xgboost(xgmodel, test)
test.label<-as.numeric(test[,2])-1
print(LogLossBinary(test.label,pred))
#load data
# set wd at the file location
load("../../../data/new3_data_all.RData")
data.all = data.all[,-c(7:10)]
##############################################
####Tune model and choose parameters
library(xgboost)
#split 80% of the data to train set and 20% to test set
smp_size <- floor(0.8 * nrow(data.all))
set.seed(123)
train_ind <- sample(seq_len(nrow(data.all)), size = smp_size)
train <- data.all[train_ind, ]
test <- data.all[-train_ind, ]
labels <- train$is_churn
test_label <- test$is_churn
trainnn<-as.matrix(train[,-1])
testtt<-as.matrix(test[,-1])
m <- mapply(trainnn, FUN=as.numeric)
m <- matrix(data=m, nrow=4000)
m.test <- mapply(testtt, FUN=as.numeric)
m.test <- matrix(data=m.test, nrow=1000)
dtrain=xgb.DMatrix(data=m[,-1],label=m[,1])
NROUNDS = c(500,1000)
ETA = c(0.3)
MAX_DEPTH = c(3,4,5,6)
cv.xgb <- function(X.train, y.train, K, NROUNDS, ETA, MAX_DEPTH){
for (nround in NROUNDS){
for (eta in ETA){
for (max_depth in MAX_DEPTH){
n <- length(y.train)
n.fold <- floor(n/K)
s <- sample(rep(1:K, c(rep(n.fold, K-1), n-(K-1)*n.fold)))
cv.loss <- rep(NA, K)
for (i in 1:K){
train.data <- X.train[s != i,]
train.label <- y.train[s != i]
test.data <- X.train[s == i,]
test.label <- y.train[s == i]
param <- list("objective" = "binary:logistic",
"eval_metric" = "mlogloss",
'eta' = eta, 'max_depth' = max_depth)
dtrain=xgb.DMatrix(data=train.data,label=train.label)
bst <- xgb.train(data = dtrain,  param = param, nrounds = nround)
pred <- predict(bst, newdata = test.data)
#cv.acc[i] <- mean(pred == test.label)
cv.loss[i]<-LogLossBinary(test.label,pred)
}
print(paste("------Mean 5-fold cv loss for nround=",nround,",eta=",eta,",max_depth=",max_depth,
"------",mean(cv.loss)))
#key = c(nround,eta,max_depth)
#CV_ERRORS[key] = mean(cv.loss)
}
}
}
}
LogLossBinary <- function(actual, predicted, eps = 1e-15) {
predicted <- pmin(pmax(predicted, eps), 1-eps)
error<- - (sum(actual * log(predicted) + (1 - actual) * log(1 - predicted))) / length(actual)
return(error)
}
#CV_ERRORS = list()
cv.xgb(m[,-1],m[,1], 5, NROUNDS, ETA, MAX_DEPTH)
#####Tuned xgboost model
source("./xgboost_train&test_nomember.R")
xgmodel = train.xgboost(train)
pred<-test.xgboost(xgmodel, test)
#####Tuned xgboost model
source("./xgboost_train&test_nomember.R")
setwd("~/Documents/GitHub/fall2017-project5-proj5-grp2/lib/no_member_data_version/xgboost")
#####Tuned xgboost model
source("./xgboost_train&test_nomember.R")
xgmodel = train.xgboost(train)
pred<-test.xgboost(xgmodel, test)
test.label<-as.numeric(test[,2])-1
print(LogLossBinary(test.label,pred))
library(adabag)
load("../../../data/new3_data_all.Rdata")
data.all = data.all[,-(7:11)]
library(adabag)
load("../../../data/new3_data_all.Rdata")
data.all = data.all[,-(7:10)]
str(data.all)
#split 80% of the data to train set and 20% to test set
smp_size <- floor(0.8 * nrow(data.all))
set.seed(123)
train_ind <- sample(seq_len(nrow(data.all)), size = smp_size)
train <- data.all[train_ind, ]
test <- data.all[-train_ind, ]
row.names(train)<-train[,1]
train<-train[,-1]
