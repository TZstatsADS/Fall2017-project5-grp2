######### Train Function & Test Function for XGBOOST############
###################################################################
library("xgboost")
source("./xgboost_train&test.R")
load("../data/data.all.RData")
data.all = data.all[,-c(1,8,9,10,11)]
smp_size <- floor(0.8 * nrow(data.all))
set.seed(123)
train_ind <- sample(seq_len(nrow(data.all)), size = smp_size)
train <- data.all[train_ind, ]
test <- data.all[-train_ind, ]
xgboost.model = train.xgboost(train.data = train)
library("xgboost")
source("./xgboost_train&test.R")
load("../data/data.all.RData")
data.all = data.all[,-c(1,8,9,10,11)]
smp_size <- floor(0.8 * nrow(data.all))
set.seed(123)
train_ind <- sample(seq_len(nrow(data.all)), size = smp_size)
train <- data.all[train_ind, ]
test <- data.all[-train_ind, ]
xgboost.model = train.xgboost(train)
trainnn<-as.matrix(train)
testtt<-as.matrix(test)
dtrain=xgb.DMatrix(data=trainnn[,-1],label=trainnn[,1])
smp_size <- floor(0.8 * nrow(data.all))
library("xgboost")
source("./xgboost_train&test.R")
load("../data/data.all.RData")
data.all = data.all[,-c(1,8,9,10,11)]
smp_size <- floor(0.8 * nrow(data.all))
set.seed(123)
train_ind <- sample(seq_len(nrow(data.all)), size = smp_size)
train <- data.all[train_ind, ]
test <- data.all[-train_ind, ]
trainnn<-as.matrix(train)
testtt<-as.matrix(test)
dtrain=xgb.DMatrix(data=trainnn[,-1],label=trainnn[,1])
dtrain=xgb.DMatrix(data=trainnn[,-1],label=trainnn[,1])
?xgb.DMatrix
xgboost.model <- train.xgboost(train)
xgboost.model <- train.xgboost(train)
xgboost.model <- train.xgboost(train)
View(train)
trainnn<-as.matrix(train[,-1])
m <- mapply(trainnn, FUN=as.numeric)
m <- matrix(data=m, nrow=4000)
View(m)
trainnn<-as.matrix(train)
m <- mapply(trainnn, FUN=as.numeric)
m <- matrix(data=m, nrow=4000)
library("xgboost")
source("./xgboost_train&test.R")
load("../data/data.all.RData")
data.all = data.all[,-c(1,8,9,10,11)]
smp_size <- floor(0.8 * nrow(data.all))
set.seed(123)
train_ind <- sample(seq_len(nrow(data.all)), size = smp_size)
train <- data.all[train_ind, ]
test <- data.all[-train_ind, ]
xgboost.model <- train.xgboost(train)
xgboost.model
xgboost.pre = test.xgboost(xgboost.model,test)
xgboost.pre = test.xgboost(xgboost.model)
xgboost.pre = test.xgboost(test)
xgboost.pre = test.xgboost(test)
xgboost.model <- train.xgboost(train)
xgboost.pre = test.xgboost(test)
xgboost.model <- train.xgboost(train)
xgboost.pre = test.xgboost(xgboost.model,test)
library("xgboost")
source("./xgboost_train&test.R")
load("../data/data.all.RData")
data.all = data.all[,-c(1,8,9,10,11)]
smp_size <- floor(0.8 * nrow(data.all))
set.seed(123)
train_ind <- sample(seq_len(nrow(data.all)), size = smp_size)
train <- data.all[train_ind, ]
test <- data.all[-train_ind, ]
xgboost.model <- train.xgboost(train)
xgboost.pre = test.xgboost(xgboost.model)
xgboost.pre = test.xgboost(xgboost.model,test)
LogLossBinary <- function(actual, predicted, eps = 1e-15) {
predicted <- pmin(pmax(predicted, eps), 1-eps)
error<- - (sum(actual * log(predicted) + (1 - actual) * log(1 - predicted))) / length(actual)
return(error)
}
View(test)
test.label<-as.numeric(test[,1])-1
LogLossBinary(test.label,pred)
LogLossBinary(test.label,xgboost.pre)
test.label
test.label<-as.numeric(test[,1])
test.label
LogLossBinary(test.label,xgboost.pre)
#load data
load("../data/new3_data_all.RData")
data.all = data.all[,-c(1,8,9,10,11)]
#load data
load("../data/new3_data_all.RData")
data.all = data.all[,-c(8,9,10,11)]
##############################################
####Tune model and choose parameters
library(xgboost)
#split 80% of the data to train set and 20% to test set
smp_size <- floor(0.8 * nrow(data.all))
set.seed(123)
train_ind <- sample(seq_len(nrow(data.all)), size = smp_size)
train <- data.all[train_ind, ]
test <- data.all[-train_ind, ]
View(train)
labels <- train$is_churn
test_label <- test$is_churn
trainnn<-as.matrix(train[,-1])
testtt<-as.matrix(test[,-1])
m <- mapply(trainnn, FUN=as.numeric)
m <- matrix(data=m, nrow=4000)
m.test <- mapply(testtt, FUN=as.numeric)
m.test <- matrix(data=m.test, nrow=1000)
dtrain=xgb.DMatrix(data=m[,-1],label=m[,1])
NROUNDS = c(500,1000)
ETA = c(0.3)
MAX_DEPTH = c(3,4,5,6)
cv.xgb <- function(X.train, y.train, K, NROUNDS, ETA, MAX_DEPTH){
for (nround in NROUNDS){
for (eta in ETA){
for (max_depth in MAX_DEPTH){
n <- length(y.train)
n.fold <- floor(n/K)
s <- sample(rep(1:K, c(rep(n.fold, K-1), n-(K-1)*n.fold)))
cv.loss <- rep(NA, K)
for (i in 1:K){
train.data <- X.train[s != i,]
train.label <- y.train[s != i]
test.data <- X.train[s == i,]
test.label <- y.train[s == i]
param <- list("objective" = "binary:logistic",
"eval_metric" = "mlogloss",
'eta' = eta, 'max_depth' = max_depth)
dtrain=xgb.DMatrix(data=train.data,label=train.label)
bst <- xgb.train(data = dtrain,  param = param, nrounds = nround)
pred <- predict(bst, newdata = test.data)
#cv.acc[i] <- mean(pred == test.label)
cv.loss[i]<-LogLossBinary(test.label,pred)
}
print(paste("------Mean 5-fold cv loss for nround=",nround,",eta=",eta,",max_depth=",max_depth,
"------",mean(cv.loss)))
#key = c(nround,eta,max_depth)
#CV_ERRORS[key] = mean(cv.loss)
}
}
}
}
#CV_ERRORS = list()
cv.xgb(m[,-1],m[,1], 5, NROUNDS, ETA, MAX_DEPTH)
#load data
load("../data/new3_data_all.RData")
data.all = data.all[,-c(8,9,10,11)]
##############################################
####Tune model and choose parameters
library(xgboost)
#split 80% of the data to train set and 20% to test set
smp_size <- floor(0.8 * nrow(data.all))
set.seed(123)
train_ind <- sample(seq_len(nrow(data.all)), size = smp_size)
train <- data.all[train_ind, ]
test <- data.all[-train_ind, ]
labels <- train$is_churn
test_label <- test$is_churn
trainnn<-as.matrix(train[,-1])
testtt<-as.matrix(test[,-1])
m <- mapply(trainnn, FUN=as.numeric)
m <- matrix(data=m, nrow=4000)
m.test <- mapply(testtt, FUN=as.numeric)
m.test <- matrix(data=m.test, nrow=1000)
dtrain=xgb.DMatrix(data=m[,-1],label=m[,1])
NROUNDS = c(500,1000)
ETA = c(0.3)
MAX_DEPTH = c(3,4,5,6)
cv.xgb <- function(X.train, y.train, K, NROUNDS, ETA, MAX_DEPTH){
for (nround in NROUNDS){
for (eta in ETA){
for (max_depth in MAX_DEPTH){
n <- length(y.train)
n.fold <- floor(n/K)
s <- sample(rep(1:K, c(rep(n.fold, K-1), n-(K-1)*n.fold)))
cv.loss <- rep(NA, K)
for (i in 1:K){
train.data <- X.train[s != i,]
train.label <- y.train[s != i]
test.data <- X.train[s == i,]
test.label <- y.train[s == i]
param <- list("objective" = "binary:logistic",
"eval_metric" = "mlogloss",
'eta' = eta, 'max_depth' = max_depth)
dtrain=xgb.DMatrix(data=train.data,label=train.label)
bst <- xgb.train(data = dtrain,  param = param, nrounds = nround)
pred <- predict(bst, newdata = test.data)
#cv.acc[i] <- mean(pred == test.label)
cv.loss[i]<-LogLossBinary(test.label,pred)
}
print(paste("------Mean 5-fold cv loss for nround=",nround,",eta=",eta,",max_depth=",max_depth,
"------",mean(cv.loss)))
#key = c(nround,eta,max_depth)
#CV_ERRORS[key] = mean(cv.loss)
}
}
}
}
#CV_ERRORS = list()
cv.xgb(m[,-1],m[,1], 5, NROUNDS, ETA, MAX_DEPTH)
cv.loss.xgboost<-LogLossBinary(test.data[,1],pred)
LogLossBinary <- function(actual, predicted, eps = 1e-15) {
predicted <- pmin(pmax(predicted, eps), 1-eps)
error<- - (sum(actual * log(predicted) + (1 - actual) * log(1 - predicted))) / length(actual)
return(error)
}
#load data
load("../data/new3_data_all.RData")
data.all[,-c(8:11)]
data.all = data.all[,-c(8:11)]
##############################################
####Tune model and choose parameters
library(xgboost)
#split 80% of the data to train set and 20% to test set
smp_size <- floor(0.8 * nrow(data.all))
set.seed(123)
train_ind <- sample(seq_len(nrow(data.all)), size = smp_size)
train <- data.all[train_ind, ]
test <- data.all[-train_ind, ]
labels <- train$is_churn
test_label <- test$is_churn
trainnn<-as.matrix(train[,-1])
testtt<-as.matrix(test[,-1])
m <- mapply(trainnn, FUN=as.numeric)
m <- matrix(data=m, nrow=4000)
m.test <- mapply(testtt, FUN=as.numeric)
m.test <- matrix(data=m.test, nrow=1000)
dtrain=xgb.DMatrix(data=m[,-1],label=m[,1])
NROUNDS = c(500,1000)
ETA = c(0.3)
MAX_DEPTH = c(3,4,5,6)
cv.xgb <- function(X.train, y.train, K, NROUNDS, ETA, MAX_DEPTH){
for (nround in NROUNDS){
for (eta in ETA){
for (max_depth in MAX_DEPTH){
n <- length(y.train)
n.fold <- floor(n/K)
s <- sample(rep(1:K, c(rep(n.fold, K-1), n-(K-1)*n.fold)))
cv.loss <- rep(NA, K)
for (i in 1:K){
train.data <- X.train[s != i,]
train.label <- y.train[s != i]
test.data <- X.train[s == i,]
test.label <- y.train[s == i]
param <- list("objective" = "binary:logistic",
"eval_metric" = "mlogloss",
'eta' = eta, 'max_depth' = max_depth)
dtrain=xgb.DMatrix(data=train.data,label=train.label)
bst <- xgb.train(data = dtrain,  param = param, nrounds = nround)
pred <- predict(bst, newdata = test.data)
#cv.acc[i] <- mean(pred == test.label)
cv.loss[i]<-LogLossBinary(test.label,pred)
}
print(paste("------Mean 5-fold cv loss for nround=",nround,",eta=",eta,",max_depth=",max_depth,
"------",mean(cv.loss)))
#key = c(nround,eta,max_depth)
#CV_ERRORS[key] = mean(cv.loss)
}
}
}
}
#CV_ERRORS = list()
cv.xgb(m[,-1],m[,1], 5, NROUNDS, ETA, MAX_DEPTH)
#CV_ERRORS = list()
cv.xgb(m[,-1],m[,1], 5, NROUNDS, ETA, MAX_DEPTH)
LogLossBinary <- function(actual, predicted, eps = 1e-15) {
predicted <- pmin(pmax(predicted, eps), 1-eps)
error<- - (sum(actual * log(predicted) + (1 - actual) * log(1 - predicted))) / length(actual)
return(error)
}
#CV_ERRORS = list()
cv.xgb(m[,-1],m[,1], 5, NROUNDS, ETA, MAX_DEPTH)
#CV_ERRORS = list()
cv.xgb(m[,-1],m[,1], 5, NROUNDS, ETA, MAX_DEPTH)
cv.loss.xgboost<-LogLossBinary(test.data[,1],pred)
#load data
source("./xgboost_train&test.R")
xgboost.model <- train.xgboost(train)
xgboost.model <- train.xgboost(train,0.3,6,500)
xgboost.model <- train.xgboost(train,0.3,6,500)
#load data
source("./xgboost_train&test.R")
load("../data/new3_data_all.RData")
data.all = data.all[,-c(8:11)]
##############################################
####Tune model and choose parameters
library(xgboost)
#split 80% of the data to train set and 20% to test set
smp_size <- floor(0.8 * nrow(data.all))
set.seed(123)
train_ind <- sample(seq_len(nrow(data.all)), size = smp_size)
train <- data.all[train_ind, ]
test <- data.all[-train_ind, ]
labels <- train$is_churn
test_label <- test$is_churn
trainnn<-as.matrix(train[,-1])
testtt<-as.matrix(test[,-1])
m <- mapply(trainnn, FUN=as.numeric)
m <- matrix(data=m, nrow=4000)
m.test <- mapply(testtt, FUN=as.numeric)
m.test <- matrix(data=m.test, nrow=1000)
dtrain=xgb.DMatrix(data=m[,-1],label=m[,1])
NROUNDS = c(500,1000)
ETA = c(0.3)
MAX_DEPTH = c(3,4,5,6)
cv.xgb <- function(X.train, y.train, K, NROUNDS, ETA, MAX_DEPTH){
for (nround in NROUNDS){
for (eta in ETA){
for (max_depth in MAX_DEPTH){
n <- length(y.train)
n.fold <- floor(n/K)
s <- sample(rep(1:K, c(rep(n.fold, K-1), n-(K-1)*n.fold)))
cv.loss <- rep(NA, K)
for (i in 1:K){
train.data <- X.train[s != i,]
train.label <- y.train[s != i]
test.data <- X.train[s == i,]
test.label <- y.train[s == i]
param <- list("objective" = "binary:logistic",
"eval_metric" = "mlogloss",
'eta' = eta, 'max_depth' = max_depth)
dtrain=xgb.DMatrix(data=train.data,label=train.label)
bst <- xgb.train(data = dtrain,  param = param, nrounds = nround)
pred <- predict(bst, newdata = test.data)
#cv.acc[i] <- mean(pred == test.label)
cv.loss[i]<-LogLossBinary(test.label,pred)
}
print(paste("------Mean 5-fold cv loss for nround=",nround,",eta=",eta,",max_depth=",max_depth,
"------",mean(cv.loss)))
#key = c(nround,eta,max_depth)
#CV_ERRORS[key] = mean(cv.loss)
}
}
}
}
LogLossBinary <- function(actual, predicted, eps = 1e-15) {
predicted <- pmin(pmax(predicted, eps), 1-eps)
error<- - (sum(actual * log(predicted) + (1 - actual) * log(1 - predicted))) / length(actual)
return(error)
}
#CV_ERRORS = list()
cv.xgb(m[,-1],m[,1], 5, NROUNDS, ETA, MAX_DEPTH)
xgboost.model <- train.xgboost(train,0.3,5,500)
xgboost.model <- train.xgboost(train,0.3,5,500)
xgboost.model <- train.xgboost(train,0.3,5,500)
xgboost.model <- train.xgboost(train,0.3,5,500)
#### data 3 svm
#### Sijian
library("e1071")
load("../data/new3_data_all.rdata")
View(data.all)
colnames(data.all)
data.all = data.all[,-c(1,9,10,15)]
colnames(data.all)[1] = "y"
#split 80% of the data to train set and 20% to test set
smp_size <- floor(0.8 * nrow(data.all))
set.seed(123)
train_ind <- sample(seq_len(nrow(data.all)), size = smp_size)
train <- data.all[train_ind, ]
test <- data.all[-train_ind, ]
######################################### tune SVM
train1 <- mapply(train, FUN=as.numeric)
train1 <- matrix(data=train1, nrow=4000)
svm_tune <- tune(svm, y~. ,data = train,
kernel="radial", ranges=list(cost=10^(-1:2), gamma=c(.5,1,2)))
print(svm_tune)
########################################## SVM training and predicting
train.svm<- function(traindata) {
traindata$y<- as.factor(traindata$y)
model.svm<- svm(y~., data = traindata, cost = 10, gamma=0.5 ,probability = TRUE)
return(model.svm)
}
test.svm <- function(model,test.data)
{
return(predict(model,test.data,probability = TRUE))
}
svm.model <- train.svm(train)
svm.pre = test.svm(svm.model,test[,-1])
####################################### performance
prob = attr(svm.pre,"probabilities")
LogLossBinary <- function(actual, predicted, eps = 1e-15) {
predicted <- pmin(pmax(predicted, eps), 1-eps)
error<- - (sum(actual * log(predicted) + (1 - actual) * log(1 - predicted))) / length(actual)
return(error)
}
testtt<-as.numeric(test[,1])-1
LogLossBinary(testtt,prob[,2])
#### data 3 svm
#### Sijian
library("e1071")
load("../data/new3_data_all.rdata")
colnames(data.all)
#### data 3 svm
#### Sijian
library("e1071")
load("../data/new3_data_all.rdata")
data.all = data.all[,-c(1,9,10,14)]
colnames(data.all)[1] = "y"
#split 80% of the data to train set and 20% to test set
smp_size <- floor(0.8 * nrow(data.all))
set.seed(123)
train_ind <- sample(seq_len(nrow(data.all)), size = smp_size)
train <- data.all[train_ind, ]
test <- data.all[-train_ind, ]
######################################### tune SVM
train1 <- mapply(train, FUN=as.numeric)
train1 <- matrix(data=train1, nrow=4000)
svm_tune <- tune(svm, y~. ,data = train,
kernel="radial", ranges=list(cost=10^(-1:2), gamma=c(.5,1,2)))
print(svm_tune)
########################################## SVM training and predicting
train.svm<- function(traindata) {
traindata$y<- as.factor(traindata$y)
model.svm<- svm(y~., data = traindata, cost = 10, gamma=0.5 ,probability = TRUE)
return(model.svm)
}
test.svm <- function(model,test.data)
{
return(predict(model,test.data,probability = TRUE))
}
svm.model <- train.svm(train)
svm.pre = test.svm(svm.model,test[,-1])
####################################### performance
prob = attr(svm.pre,"probabilities")
LogLossBinary <- function(actual, predicted, eps = 1e-15) {
predicted <- pmin(pmax(predicted, eps), 1-eps)
error<- - (sum(actual * log(predicted) + (1 - actual) * log(1 - predicted))) / length(actual)
return(error)
}
testtt<-as.numeric(test[,1])-1
LogLossBinary(testtt,prob[,2])
colnames(data.all)
#### data 3 svm
#### Sijian
library("e1071")
load("../data/new3_data_all.rdata")
colnames(data.all)
data.all = data.all[,-c(1,9,10,15)]
colnames(data.all)[1] = "y"
#split 80% of the data to train set and 20% to test set
smp_size <- floor(0.8 * nrow(data.all))
set.seed(123)
train_ind <- sample(seq_len(nrow(data.all)), size = smp_size)
train <- data.all[train_ind, ]
test <- data.all[-train_ind, ]
######################################### tune SVM
train1 <- mapply(train, FUN=as.numeric)
train1 <- matrix(data=train1, nrow=4000)
svm_tune <- tune(svm, y~. ,data = train,
kernel="radial", ranges=list(cost=10^(-1:2), gamma=c(.5,1,2)))
print(svm_tune)
########################################## SVM training and predicting
train.svm<- function(traindata) {
traindata$y<- as.factor(traindata$y)
model.svm<- svm(y~., data = traindata, cost = 10, gamma=0.5 ,probability = TRUE)
return(model.svm)
}
test.svm <- function(model,test.data)
{
return(predict(model,test.data,probability = TRUE))
}
svm.model <- train.svm(train)
svm.pre = test.svm(svm.model,test[,-1])
####################################### performance
prob = attr(svm.pre,"probabilities")
LogLossBinary <- function(actual, predicted, eps = 1e-15) {
predicted <- pmin(pmax(predicted, eps), 1-eps)
error<- - (sum(actual * log(predicted) + (1 - actual) * log(1 - predicted))) / length(actual)
return(error)
}
testtt<-as.numeric(test[,1])-1
LogLossBinary(testtt,prob[,2])
load("/Users/sijianxuan/Documents/GitHub/fall2017-project5-proj5-grp2/data/data.all.RData")
View(data.all)
###Feature Importance using XGBoost
library(xgboost)
a<-train.xgboost(train)
###Feature Importance using XGBoost
library(xgboost)
source("./Train_and_Test.R")
a<-train.xgboost(train)
###Feature Importance using XGBoost
library(xgboost)
source("./Train_and_Test.R")
#### data 3 svm
#### Sijian
library("e1071")
load("../data/new3_data_all.rdata")
#split 80% of the data to train set and 20% to test set
smp_size <- floor(0.8 * nrow(data.all))
set.seed(123)
train_ind <- sample(seq_len(nrow(data.all)), size = smp_size)
train <- data.all[train_ind, ]
test <- data.all[-train_ind, ]
a<-train.xgboost(train)
varnames <- setdiff(colnames(train), c("msno", "is_churn"))
names <- dimnames(train[,c(-1,-2)])[[2]]
importance_matrix <- xgb.importance(names, model=a)
xgb.plot.importance(importance_matrix)
importance_matrix
?xgb.importance
