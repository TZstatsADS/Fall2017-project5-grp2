setwd("~/Documents/GitHub/fall2017-project5-proj5-grp2/lib")
source("./xgboost_train&test.R")
load("../data/data.all.RData")
View(data.all)
data_wo_member = data.all[,-c(1,8,9,10,11)]
View(data_wo_member)
source("./xgboost_train&test.R")
load("../data/data.all.RData")
data.all = data.all[,-c(1,8,9,10,11)]
smp_size <- floor(0.8 * nrow(data.all))
set.seed(123)
train_ind <- sample(seq_len(nrow(data.all)), size = smp_size)
train <- data.all[train_ind, ]
test <- data.all[-train_ind, ]
xgboost.model = train.xgboost(train.data = train)
xgboost.model = train.xgboost(train.data = train)
######### Train Function & Test Function for XGBOOST############
###################################################################
library("xgboost")
source("./xgboost_train&test.R")
load("../data/data.all.RData")
data.all = data.all[,-c(1,8,9,10,11)]
smp_size <- floor(0.8 * nrow(data.all))
set.seed(123)
train_ind <- sample(seq_len(nrow(data.all)), size = smp_size)
train <- data.all[train_ind, ]
test <- data.all[-train_ind, ]
xgboost.model = train.xgboost(train.data = train)
library("xgboost")
source("./xgboost_train&test.R")
load("../data/data.all.RData")
data.all = data.all[,-c(1,8,9,10,11)]
smp_size <- floor(0.8 * nrow(data.all))
set.seed(123)
train_ind <- sample(seq_len(nrow(data.all)), size = smp_size)
train <- data.all[train_ind, ]
test <- data.all[-train_ind, ]
xgboost.model = train.xgboost(train)
trainnn<-as.matrix(train)
testtt<-as.matrix(test)
dtrain=xgb.DMatrix(data=trainnn[,-1],label=trainnn[,1])
smp_size <- floor(0.8 * nrow(data.all))
library("xgboost")
source("./xgboost_train&test.R")
load("../data/data.all.RData")
data.all = data.all[,-c(1,8,9,10,11)]
smp_size <- floor(0.8 * nrow(data.all))
set.seed(123)
train_ind <- sample(seq_len(nrow(data.all)), size = smp_size)
train <- data.all[train_ind, ]
test <- data.all[-train_ind, ]
trainnn<-as.matrix(train)
testtt<-as.matrix(test)
dtrain=xgb.DMatrix(data=trainnn[,-1],label=trainnn[,1])
dtrain=xgb.DMatrix(data=trainnn[,-1],label=trainnn[,1])
?xgb.DMatrix
xgboost.model <- train.xgboost(train)
xgboost.model <- train.xgboost(train)
xgboost.model <- train.xgboost(train)
View(train)
trainnn<-as.matrix(train[,-1])
m <- mapply(trainnn, FUN=as.numeric)
m <- matrix(data=m, nrow=4000)
View(m)
trainnn<-as.matrix(train)
m <- mapply(trainnn, FUN=as.numeric)
m <- matrix(data=m, nrow=4000)
library("xgboost")
source("./xgboost_train&test.R")
load("../data/data.all.RData")
data.all = data.all[,-c(1,8,9,10,11)]
smp_size <- floor(0.8 * nrow(data.all))
set.seed(123)
train_ind <- sample(seq_len(nrow(data.all)), size = smp_size)
train <- data.all[train_ind, ]
test <- data.all[-train_ind, ]
xgboost.model <- train.xgboost(train)
xgboost.model
xgboost.pre = test.xgboost(xgboost.model,test)
xgboost.pre = test.xgboost(xgboost.model)
xgboost.pre = test.xgboost(test)
xgboost.pre = test.xgboost(test)
xgboost.model <- train.xgboost(train)
xgboost.pre = test.xgboost(test)
xgboost.model <- train.xgboost(train)
xgboost.pre = test.xgboost(xgboost.model,test)
library("xgboost")
source("./xgboost_train&test.R")
load("../data/data.all.RData")
data.all = data.all[,-c(1,8,9,10,11)]
smp_size <- floor(0.8 * nrow(data.all))
set.seed(123)
train_ind <- sample(seq_len(nrow(data.all)), size = smp_size)
train <- data.all[train_ind, ]
test <- data.all[-train_ind, ]
xgboost.model <- train.xgboost(train)
xgboost.pre = test.xgboost(xgboost.model)
xgboost.pre = test.xgboost(xgboost.model,test)
LogLossBinary <- function(actual, predicted, eps = 1e-15) {
predicted <- pmin(pmax(predicted, eps), 1-eps)
error<- - (sum(actual * log(predicted) + (1 - actual) * log(1 - predicted))) / length(actual)
return(error)
}
View(test)
test.label<-as.numeric(test[,1])-1
LogLossBinary(test.label,pred)
LogLossBinary(test.label,xgboost.pre)
test.label
test.label<-as.numeric(test[,1])
test.label
LogLossBinary(test.label,xgboost.pre)
#load data
load("../data/new3_data_all.RData")
data.all = data.all[,-c(1,8,9,10,11)]
#load data
load("../data/new3_data_all.RData")
data.all = data.all[,-c(8,9,10,11)]
##############################################
####Tune model and choose parameters
library(xgboost)
#split 80% of the data to train set and 20% to test set
smp_size <- floor(0.8 * nrow(data.all))
set.seed(123)
train_ind <- sample(seq_len(nrow(data.all)), size = smp_size)
train <- data.all[train_ind, ]
test <- data.all[-train_ind, ]
View(train)
labels <- train$is_churn
test_label <- test$is_churn
trainnn<-as.matrix(train[,-1])
testtt<-as.matrix(test[,-1])
m <- mapply(trainnn, FUN=as.numeric)
m <- matrix(data=m, nrow=4000)
m.test <- mapply(testtt, FUN=as.numeric)
m.test <- matrix(data=m.test, nrow=1000)
dtrain=xgb.DMatrix(data=m[,-1],label=m[,1])
NROUNDS = c(500,1000)
ETA = c(0.3)
MAX_DEPTH = c(3,4,5,6)
cv.xgb <- function(X.train, y.train, K, NROUNDS, ETA, MAX_DEPTH){
for (nround in NROUNDS){
for (eta in ETA){
for (max_depth in MAX_DEPTH){
n <- length(y.train)
n.fold <- floor(n/K)
s <- sample(rep(1:K, c(rep(n.fold, K-1), n-(K-1)*n.fold)))
cv.loss <- rep(NA, K)
for (i in 1:K){
train.data <- X.train[s != i,]
train.label <- y.train[s != i]
test.data <- X.train[s == i,]
test.label <- y.train[s == i]
param <- list("objective" = "binary:logistic",
"eval_metric" = "mlogloss",
'eta' = eta, 'max_depth' = max_depth)
dtrain=xgb.DMatrix(data=train.data,label=train.label)
bst <- xgb.train(data = dtrain,  param = param, nrounds = nround)
pred <- predict(bst, newdata = test.data)
#cv.acc[i] <- mean(pred == test.label)
cv.loss[i]<-LogLossBinary(test.label,pred)
}
print(paste("------Mean 5-fold cv loss for nround=",nround,",eta=",eta,",max_depth=",max_depth,
"------",mean(cv.loss)))
#key = c(nround,eta,max_depth)
#CV_ERRORS[key] = mean(cv.loss)
}
}
}
}
#CV_ERRORS = list()
cv.xgb(m[,-1],m[,1], 5, NROUNDS, ETA, MAX_DEPTH)
#load data
load("../data/new3_data_all.RData")
data.all = data.all[,-c(8,9,10,11)]
##############################################
####Tune model and choose parameters
library(xgboost)
#split 80% of the data to train set and 20% to test set
smp_size <- floor(0.8 * nrow(data.all))
set.seed(123)
train_ind <- sample(seq_len(nrow(data.all)), size = smp_size)
train <- data.all[train_ind, ]
test <- data.all[-train_ind, ]
labels <- train$is_churn
test_label <- test$is_churn
trainnn<-as.matrix(train[,-1])
testtt<-as.matrix(test[,-1])
m <- mapply(trainnn, FUN=as.numeric)
m <- matrix(data=m, nrow=4000)
m.test <- mapply(testtt, FUN=as.numeric)
m.test <- matrix(data=m.test, nrow=1000)
dtrain=xgb.DMatrix(data=m[,-1],label=m[,1])
NROUNDS = c(500,1000)
ETA = c(0.3)
MAX_DEPTH = c(3,4,5,6)
cv.xgb <- function(X.train, y.train, K, NROUNDS, ETA, MAX_DEPTH){
for (nround in NROUNDS){
for (eta in ETA){
for (max_depth in MAX_DEPTH){
n <- length(y.train)
n.fold <- floor(n/K)
s <- sample(rep(1:K, c(rep(n.fold, K-1), n-(K-1)*n.fold)))
cv.loss <- rep(NA, K)
for (i in 1:K){
train.data <- X.train[s != i,]
train.label <- y.train[s != i]
test.data <- X.train[s == i,]
test.label <- y.train[s == i]
param <- list("objective" = "binary:logistic",
"eval_metric" = "mlogloss",
'eta' = eta, 'max_depth' = max_depth)
dtrain=xgb.DMatrix(data=train.data,label=train.label)
bst <- xgb.train(data = dtrain,  param = param, nrounds = nround)
pred <- predict(bst, newdata = test.data)
#cv.acc[i] <- mean(pred == test.label)
cv.loss[i]<-LogLossBinary(test.label,pred)
}
print(paste("------Mean 5-fold cv loss for nround=",nround,",eta=",eta,",max_depth=",max_depth,
"------",mean(cv.loss)))
#key = c(nround,eta,max_depth)
#CV_ERRORS[key] = mean(cv.loss)
}
}
}
}
#CV_ERRORS = list()
cv.xgb(m[,-1],m[,1], 5, NROUNDS, ETA, MAX_DEPTH)
cv.loss.xgboost<-LogLossBinary(test.data[,1],pred)
LogLossBinary <- function(actual, predicted, eps = 1e-15) {
predicted <- pmin(pmax(predicted, eps), 1-eps)
error<- - (sum(actual * log(predicted) + (1 - actual) * log(1 - predicted))) / length(actual)
return(error)
}
#load data
load("../data/new3_data_all.RData")
data.all[,-c(8:11)]
data.all = data.all[,-c(8:11)]
##############################################
####Tune model and choose parameters
library(xgboost)
#split 80% of the data to train set and 20% to test set
smp_size <- floor(0.8 * nrow(data.all))
set.seed(123)
train_ind <- sample(seq_len(nrow(data.all)), size = smp_size)
train <- data.all[train_ind, ]
test <- data.all[-train_ind, ]
labels <- train$is_churn
test_label <- test$is_churn
trainnn<-as.matrix(train[,-1])
testtt<-as.matrix(test[,-1])
m <- mapply(trainnn, FUN=as.numeric)
m <- matrix(data=m, nrow=4000)
m.test <- mapply(testtt, FUN=as.numeric)
m.test <- matrix(data=m.test, nrow=1000)
dtrain=xgb.DMatrix(data=m[,-1],label=m[,1])
NROUNDS = c(500,1000)
ETA = c(0.3)
MAX_DEPTH = c(3,4,5,6)
cv.xgb <- function(X.train, y.train, K, NROUNDS, ETA, MAX_DEPTH){
for (nround in NROUNDS){
for (eta in ETA){
for (max_depth in MAX_DEPTH){
n <- length(y.train)
n.fold <- floor(n/K)
s <- sample(rep(1:K, c(rep(n.fold, K-1), n-(K-1)*n.fold)))
cv.loss <- rep(NA, K)
for (i in 1:K){
train.data <- X.train[s != i,]
train.label <- y.train[s != i]
test.data <- X.train[s == i,]
test.label <- y.train[s == i]
param <- list("objective" = "binary:logistic",
"eval_metric" = "mlogloss",
'eta' = eta, 'max_depth' = max_depth)
dtrain=xgb.DMatrix(data=train.data,label=train.label)
bst <- xgb.train(data = dtrain,  param = param, nrounds = nround)
pred <- predict(bst, newdata = test.data)
#cv.acc[i] <- mean(pred == test.label)
cv.loss[i]<-LogLossBinary(test.label,pred)
}
print(paste("------Mean 5-fold cv loss for nround=",nround,",eta=",eta,",max_depth=",max_depth,
"------",mean(cv.loss)))
#key = c(nround,eta,max_depth)
#CV_ERRORS[key] = mean(cv.loss)
}
}
}
}
#CV_ERRORS = list()
cv.xgb(m[,-1],m[,1], 5, NROUNDS, ETA, MAX_DEPTH)
#CV_ERRORS = list()
cv.xgb(m[,-1],m[,1], 5, NROUNDS, ETA, MAX_DEPTH)
LogLossBinary <- function(actual, predicted, eps = 1e-15) {
predicted <- pmin(pmax(predicted, eps), 1-eps)
error<- - (sum(actual * log(predicted) + (1 - actual) * log(1 - predicted))) / length(actual)
return(error)
}
#CV_ERRORS = list()
cv.xgb(m[,-1],m[,1], 5, NROUNDS, ETA, MAX_DEPTH)
#CV_ERRORS = list()
cv.xgb(m[,-1],m[,1], 5, NROUNDS, ETA, MAX_DEPTH)
cv.loss.xgboost<-LogLossBinary(test.data[,1],pred)
#load data
source("./xgboost_train&test.R")
xgboost.model <- train.xgboost(train)
xgboost.model <- train.xgboost(train,0.3,6,500)
xgboost.model <- train.xgboost(train,0.3,6,500)
#load data
source("./xgboost_train&test.R")
load("../data/new3_data_all.RData")
data.all = data.all[,-c(8:11)]
##############################################
####Tune model and choose parameters
library(xgboost)
#split 80% of the data to train set and 20% to test set
smp_size <- floor(0.8 * nrow(data.all))
set.seed(123)
train_ind <- sample(seq_len(nrow(data.all)), size = smp_size)
train <- data.all[train_ind, ]
test <- data.all[-train_ind, ]
labels <- train$is_churn
test_label <- test$is_churn
trainnn<-as.matrix(train[,-1])
testtt<-as.matrix(test[,-1])
m <- mapply(trainnn, FUN=as.numeric)
m <- matrix(data=m, nrow=4000)
m.test <- mapply(testtt, FUN=as.numeric)
m.test <- matrix(data=m.test, nrow=1000)
dtrain=xgb.DMatrix(data=m[,-1],label=m[,1])
NROUNDS = c(500,1000)
ETA = c(0.3)
MAX_DEPTH = c(3,4,5,6)
cv.xgb <- function(X.train, y.train, K, NROUNDS, ETA, MAX_DEPTH){
for (nround in NROUNDS){
for (eta in ETA){
for (max_depth in MAX_DEPTH){
n <- length(y.train)
n.fold <- floor(n/K)
s <- sample(rep(1:K, c(rep(n.fold, K-1), n-(K-1)*n.fold)))
cv.loss <- rep(NA, K)
for (i in 1:K){
train.data <- X.train[s != i,]
train.label <- y.train[s != i]
test.data <- X.train[s == i,]
test.label <- y.train[s == i]
param <- list("objective" = "binary:logistic",
"eval_metric" = "mlogloss",
'eta' = eta, 'max_depth' = max_depth)
dtrain=xgb.DMatrix(data=train.data,label=train.label)
bst <- xgb.train(data = dtrain,  param = param, nrounds = nround)
pred <- predict(bst, newdata = test.data)
#cv.acc[i] <- mean(pred == test.label)
cv.loss[i]<-LogLossBinary(test.label,pred)
}
print(paste("------Mean 5-fold cv loss for nround=",nround,",eta=",eta,",max_depth=",max_depth,
"------",mean(cv.loss)))
#key = c(nround,eta,max_depth)
#CV_ERRORS[key] = mean(cv.loss)
}
}
}
}
LogLossBinary <- function(actual, predicted, eps = 1e-15) {
predicted <- pmin(pmax(predicted, eps), 1-eps)
error<- - (sum(actual * log(predicted) + (1 - actual) * log(1 - predicted))) / length(actual)
return(error)
}
#CV_ERRORS = list()
cv.xgb(m[,-1],m[,1], 5, NROUNDS, ETA, MAX_DEPTH)
xgboost.model <- train.xgboost(train,0.3,5,500)
xgboost.model <- train.xgboost(train,0.3,5,500)
xgboost.model <- train.xgboost(train,0.3,5,500)
xgboost.model <- train.xgboost(train,0.3,5,500)
